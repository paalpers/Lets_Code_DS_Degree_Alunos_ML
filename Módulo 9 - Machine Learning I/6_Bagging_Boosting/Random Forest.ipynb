{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble: Bagging x Boosting x Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conteúdo dessa aula:\n",
    "- Tipos de Ensemble\n",
    "- Bagging: Random Forest\n",
    "\n",
    "Ensemble nada mais é do que a **sabedoria da maioria**. Aqui combinamos vários modelos mais simples em um único \n",
    "modelo robusto a fim de reduzir o viés, variância e/ou aumentar a acurácia.\n",
    "<br>\n",
    "\n",
    "\n",
    "## Tipos de Ensemble:\n",
    "- __1. Bagging (short for bootstrap aggregation)__: Treina paralelamente N modelos mais fracos (geralmente do mesmo tipo - homogêneo) com __N subsets distintos__ criados com __amostragem randômica e reposição (bootstrap)__. Cada modelo é avaliado na fase de teste com o label definido pela moda (classificação) ou pela __média dos valores__ (regressão). Devido à essa agregação final que vem o aggregation do nome. Os métodos de Bagging reduzem a variância da predição. <br>\n",
    "Algoritmos  famosos: Random Forest <br>\n",
    "<img src='images/bagging.png' style=\"width:600px\"  text=\"http://cheatsheets.aqeel-anwar.com\" />  \n",
    "<br>\n",
    "<br>\n",
    "\n",
    "- __2. Boosting__: Treina N modelos mais fracos (geralmente do mesmo tipo - homogênio) de forma sequencial. Os pontos que foram classificados erroneamente recebem um peso maior para entrar no próximo modelo. Na fase de teste, cada modelo é avaliado com base do erro de teste de cada modelo, a predição é feita com um peso sobre a votação. Os métodos de Boosting reduzem o viés da predição. <br>\n",
    "Algoritmos  famosos: AdaBoost, Gradient Boosting, XGBoost, CatBoost, LightGBM (Light Gradient Boosting Machine) <br>\n",
    "<img src='images/boosting.png' style=\"width:600px\" text=\"Fonte: http://cheatsheets.aqeel-anwar.com\" />\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "- __3. Ensemble de modelos distintos__: Treina N modelos distintos, por exemplo: um Random Forest e um SVM e faz a previsão de acordo com a saída desses modelos. \n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\n",
    "- __4. Stacking__: Treina N modelos mais fracos (geralmente de tipos distintos - heterogênio) em um subset do conjunto de dados. Uma vez que os modelos foram treinados, cria-se um novo modelo (meta learning) para combinar a saída de cada um dos modelos mais fracos resultando na predição final. Isso é feito no segundo subset dos dados. Na fase de teste, cada modelo mais fraco faz sua predição independentemente e esses labels entram como features do meta learner para gerar a predição final.\n",
    "<br>\n",
    "<img src='images/stacking.png' style=\"width:600px\" text=\"Fonte: http://cheatsheets.aqeel-anwar.com\" />\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "##### Resumo:\n",
    "<img src='images/comparison_img.png' style=\"width:600px\" text=\"Fonte: https://quantdare.com/what-is-the-difference-between-bagging-and-boosting\" />\n",
    "\n",
    "<img src='images/comparison.png' style=\"width:600px\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging: [Random Forest](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)\n",
    "\n",
    "Uma técnica muito interessante baseada em árvores é o **Random Forest**. O random forest faz um ensemble de modelos de árvore de decisão, mas com duas melhorias: \n",
    "\n",
    "> Selecionamos **aleatoriamente com reposição** algumas linhas da base original. Isso gera um novo dataset (reamostrado), chamado de **bootstrapped dataset**. O número de linhas do dataset reamostrado é controlável.\n",
    "\n",
    "> __Cada vez que fazemos um split das árvores de decisão, apenas uma amostra aleatória das features é comparada para escolher o split__. A quantidade de features a serem consideradas é controlável. (max_features e bootstrap_features). \n",
    "\n",
    "<img src=\"https://www.researchgate.net/profile/Nikolaos-Sapountzoglou/publication/339447755/figure/fig1/AS:862073311469568@1582545702096/Example-of-a-random-forest.png\" width=800>\n",
    "\n",
    "Dessa forma, cada árvore será treinada em um dataset diferente (devido ao bootstrap) e assim, cada modelo cometerá erros em diferentes lugares gerando um viés e uma variância diferente para cada um. \n",
    "\n",
    "Esse processo é denominado de **bootstrapping** e ele introduz **duas fontes de aleatoriedade**, cujo objetivo é **diminuir a variância** (tendência a overfitting) do modelo.\n",
    "\n",
    "De fato, árvores individuais são facilmente overfitadas, como discutimos em aula (lembre-se da grande flexibilidade da hipótese em encontrar condições favoráveis à aprendizagem dos ruídos!).\n",
    "\n",
    "Com esta aleatorização introduzida pelo bootstrapping, o objetivo é que as árvores construídas sejam **independentes**, de modo que **os erros cometidos por cada uma sejam independentes** e dessa forma as RFs atacam o principal problema das DTs: a variância.\n",
    "\n",
    "Deste modo, se considerarmos as previsões isoladas e de alguma forma **agregar** as previsões, a expectativa é que o modelo final seja **menos propenso a overfitting**! Mas, uma pergunta natural é: o que é essa \"agregação\"? Aqui entra o segundo elemento do bagging...\n",
    "\n",
    "Como cada árvore produz **o seu target**, a **agregação** é utilizada para tomar a decisão final:\n",
    "\n",
    "> No caso de classificação, a classe final é atribuída como **a classe majoritária**, isso é, **a classe que foi o output $\\hat{y}$ mais vezes dentre todas as árvores**;\n",
    "\n",
    "> No caso de regressão, o valor final é atribuído como **a média dos valores preditos $\\hat{y}$ por cada árvore**.\n",
    "\n",
    "Note que em ambos os casos, o procedimento de agregação pode ser visto como uma **média**, e o sklearn deixa isso explícito: \"*In contrast to the original publication, the scikit-learn implementation combines classifiers by averaging their probabilistic prediction, instead of letting each classifier vote for a single class.*\"\n",
    "\n",
    "Tomando a média como procedimento de agregação, a expectativa é que **alguns erros sejam anulados**, garantindo uma previsão final **mais estável e mais generalizável**, dado que os ruídos são eliminados.\n",
    "\n",
    "Esses dois processos juntos, bootstrap e agregation, são nomeados de bagging.\n",
    "\n",
    "No final de todo o processo de construção das árvores, as features mais importantes terão uma probabilidade maior de aparecer próximas às raízes das árvores (root), enquanto features menos importantes aparecerão próximas aos nós finais (leaves). Dessa forma, é possível estimar a importância de uma feature calulando a profundidade média em que ela aparece ao longo de todas as árvores.\n",
    "\n",
    "Agora pensem: é ruim termos duas variáveis muito correlacionas nesse tipo de modelagem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pré-processamento dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-13T23:09:50.460468Z",
     "start_time": "2021-01-13T23:09:42.579330Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/bank-full.csv\") \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attribute Information:\n",
    "\n",
    "   - age (numeric)\n",
    "   - job : type of job (categorical: admin.', 'bluecollar', 'entrepreneur', 'housemaid', 'management', 'retired', 'selfemployed', 'services', 'student', 'technician', 'unemployed', 'unknown')\n",
    "   - marital : marital status (categorical:'divorced', 'married', 'single', 'unknown'; note: 'divorced' means divorced or widowed)\n",
    "   - education (categorical: 'basic.4y', 'basic.6y', 'basic.9y', 'high.school', 'illiterate', 'professional.course', 'university.degree', 'unknown')\n",
    "   - default: has credit in default? (categorical: 'no','yes','unknown')\n",
    "   - balance: average yearly balance, in euros (numeric)\n",
    "   - housing: has housing loan? (categorical: 'no','yes','unknown')\n",
    "   - loan: has personal loan? (categorical: 'no','yes','unknown')\n",
    "   - contact: contact communication type (categorical:'cellular','telephone','unknown')\n",
    "   - day: last contact day of the month (numeric 1 -31)\n",
    "   - month: last contact month of year (categorical: 'jan', 'feb','mar', …, 'nov', 'dec')\n",
    "   - duration: last contact duration, in seconds (numeric).\n",
    "    Important note: this attribute highly affects the output target (e.g., if duration=0 then y='no'). Yet, the duration is not known before a call is performed. Also, after the end of the call y is obviously known.Thus, this input should only be included for benchmark purposes and should be discarded if the intention is to have a realistic predictive model.\n",
    "   - campaign: number of contacts performed during this campaign and for this client (numeric, includes last contact)\n",
    "   - pdays: number of days that passed by after the client was last contacted from a previous campaign (numeric; 999 means client was not previously contacted)\n",
    "   - previous: number of contacts performed before this campaign and for this client (numeric)\n",
    "   - poutcome: outcome of the previous marketing campaign (categorical: 'failure', 'nonexistent', 'success')\n",
    "   - target: has the client subscribed a term deposit? (binary:\"yes\", \"no\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Separar feature e target\n",
    "y = df['Target'].copy()\n",
    "x = df.drop('Target', axis=1).copy()\n",
    "\n",
    "#### particionar dados\n",
    "from sklearn.model_selection import train_test_split \n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, stratify=y, test_size=0.2, random_state=42) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resetar os index\n",
    "x_train, x_test, y_train, y_test = x_train.reset_index(drop=True), x_test.reset_index(drop=True), y_train.reset_index(drop=True), y_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecionar as colunas categóricas e numéricas\n",
    "cat_columns = x_train.select_dtypes(['object']).columns\n",
    "num_columns = x_train.select_dtypes(exclude=['object']).columns\n",
    "cat_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printa valores de cada coluna categórica\n",
    "[print(f\"{c}: {x_train[c].unique()}\") for c in cat_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converte colunas categóricas em numéricas\n",
    "# Importa OneHotEncoder e LabelEncoder\n",
    "\n",
    "\n",
    "# Intância ambos\n",
    "\n",
    "\n",
    "# Enconding das features categóricas\n",
    "\n",
    "\n",
    "# Converte para pandas dataframe renomeando as colunas\n",
    "\n",
    "\n",
    "# Concatena colunas do encoder com as numéricas\n",
    "\n",
    "\n",
    "# Encoding dos labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temos classes desbalanceadas?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Random Forest](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-14T00:17:58.938005Z",
     "start_time": "2021-01-14T00:17:58.933966Z"
    }
   },
   "outputs": [],
   "source": [
    "# Vamos importar o modelo de classificação do Random Forest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-14T00:23:12.896000Z",
     "start_time": "2021-01-14T00:23:05.630906Z"
    }
   },
   "outputs": [],
   "source": [
    "# Instancia a classe do modelo\n",
    "model_rf = \n",
    "\n",
    "# Faz o treino na base de treino\n",
    "\n",
    "\n",
    "# Faz a predição do modelo treinado na base de teste\n",
    "y_pred_rf = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obter parâmetros utilizados no treino do modelo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obter probabilidades estimadas para cada classe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Avaliar o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-14T00:23:13.029226Z",
     "start_time": "2021-01-14T00:23:12.992545Z"
    }
   },
   "outputs": [],
   "source": [
    "# Importa classes que serão utilizadas\n",
    "\n",
    "\n",
    "# Gera o classification_report para o teste\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcula a confusion_matrix para o teste\n",
    "\n",
    "\n",
    "# Plota a confusion matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = x_train.columns\n",
    "importances = model_rf.feature_importances_\n",
    "indices = np.argsort(importances)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.title('Feature Importances')\n",
    "plt.barh(range(len(indices)), importances[indices], color='b', align='center')\n",
    "plt.yticks(range(len(indices)), [features[i] for i in indices])\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O que aconteceria aqui se tivéssemos classes muito correlacionadas?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos criar modelos menos complexos selecionando as features mais importantes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance = ['duration', 'balance', 'age', 'day', 'poutcome_success', 'pdays', 'campaign', 'housing_yes', 'previous']\n",
    "\n",
    "# Instancia modelo\n",
    "model_rfi = \n",
    "\n",
    "# Faz o fit considerando apenas as variáveis mais importantes\n",
    "\n",
    "\n",
    "# Faz o predict apenas nas variáveis mais importantes\n",
    "y_pred_rfi =\n",
    "\n",
    "# Classification report\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com um modelo bem menos complexo conseguimos um resultado próximo do modelo no qual utilizamos todas as features.\n",
    "\n",
    "_____________________________________________\n",
    "_____________________________________________\n",
    "_____________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___________\n",
    "___________\n",
    "___________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Como ficaria esse código com Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.pipeline import Pipeline as pp\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.model_selection import KFold, RandomizedSearchCV, GridSearchCV, train_test_split, cross_validate\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, stratify=y, test_size=0.2, random_state=42) \n",
    "\n",
    "importance_pipe = ['duration', 'balance', 'age', 'day', 'poutcome', 'pdays', 'campaign', 'housing', 'previous']\n",
    "cat_columns_pipe = [c for c in cat_columns if c in importance_pipe]\n",
    "\n",
    "# Encoding dos labels\n",
    "y_train = le.fit_transform(y_train)\n",
    "y_test = le.transform(y_test)\n",
    "\n",
    "# Criando pipeline das variáveis categóricas\n",
    "cat_pipe = Pipeline([\n",
    "    ('ohe', OneHotEncoder(drop='first', sparse=False, handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Para termos no modelo tanto as variáveis categóricas quanto as numéricas\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('cat', cat_pipe, cat_columns_pipe)\n",
    "], remainder='passthrough')\n",
    "\n",
    "# Criando pipeline final\n",
    "pipe = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', RandomForestClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# Tunando hiperparâmetros com 3-fold cross-validation e pipelines\n",
    "parameters = {'model__max_depth': [3, 4, 5],\n",
    "              \"model__n_estimators\" : [5, 8, 10],\n",
    "              \"model__criterion\" : [\"gini\", \"entropy\"],}\n",
    "\n",
    "kfold = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "# grid = GridSearchCV(pipe, param_grid=parameters, cv=kfold, scoring='f1', n_jobs=2)\n",
    "grid = RandomizedSearchCV(pipe, param_distributions=parameters, cv=kfold, scoring='f1', n_jobs=2)\n",
    "grid.fit(x_train[importance_pipe], y_train)\n",
    "\n",
    "# qual o melhor parâmetro\n",
    "grid.best_params_ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predizendo no x_test\n",
    "y_pred = grid.predict(x_test[importance_pipe])\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos predizer em uma única amostra de dado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data = x_test[importance_pipe].iloc[[77]]\n",
    "y_pred_ = grid.predict(my_data)\n",
    "print(my_data)\n",
    "print(y_pred_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se quisermos fazer um tratamento de classes desbalanceadas, precisamos utilizar o Pipeline do pacote imblearn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.pipeline import Pipeline as pp\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "steps = [('over', SMOTE()), \n",
    "         ('scale', MinMaxScaler()),\n",
    "         ('model', RandomForestClassifier())]\n",
    "pipeline = pp(steps=steps)\n",
    "scores = cross_validate(pipeline, x_train[num_columns], y_train, scoring='roc_auc', cv=3, n_jobs=-1)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tunando hiperparâmetros com 3-fold cross-validation e pipelines\n",
    "parameters = {'model__max_depth': [3, 4, 5],\n",
    "              \"model__n_estimators\" : [100, 150, 200],\n",
    "              \"model__criterion\" : [\"gini\", \"entropy\"],}\n",
    "\n",
    "grid_imbalanced = RandomizedSearchCV(pipeline, param_distributions=parameters, cv=3, scoring='f1', n_jobs=2)\n",
    "\n",
    "# Vamos usar só variáveis numéricas por causa do SMOTE\n",
    "importance_pipe_num = [c for c in importance if c in num_columns]\n",
    "\n",
    "grid_imbalanced.fit(x_train[importance_pipe_num], y_train)\n",
    "\n",
    "# qual o melhor parâmetro\n",
    "grid_imbalanced.best_params_ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____________________\n",
    "______________________\n",
    "________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vantagens e Desvantagens\n",
    "__Vantagens:__\n",
    "* Geralmente fornecem modelos com alta acurácia (classificação)\n",
    "* Necessita de pouco tratamento dos dados\n",
    "* Conseguem lidar com dados faltantes (dois métodos: média dos valores para repor variáveis contínuas e computa proximity-weighted average)\n",
    "* Fornece uma estimativa da importancia das features\n",
    "* São robustos aos [outliers](https://stats.stackexchange.com/questions/187200/how-are-random-forests-not-sensitive-to-outliers) nas variáveis independentes e conseguem lidar com eles automaticamente (tendem a isolar os outliers)\n",
    "* Podem ser usados na seleção de features\n",
    "* Conseguem construir fronteiras de decisão não-lineares\n",
    "* Os dados não precisam seguir uma distribuição normal (como em modelos lineares), mas vale testar!\n",
    "* Lidam bem com uma quantidade muito grande de dados e features\n",
    "\n",
    "__Desvantagens:__\n",
    "* No RF as variáveis categóricas não ordinais devem ser convertidas para dummies ([mean encoding](https://towardsdatascience.com/why-you-should-try-mean-encoding-17057262cd0))\n",
    "* Não é recomendado utilizar nenhum modelo que depende do bagging em problemas com classes desbalanceados. Em dados extremamente desbalanceados existe uma probabilidade significativa de uma amostra ser selecionada com poucas ou nenhuma amostra da classe minoritária.\n",
    "* Não é muito recomendado na extrapolação de dados (suponha a predição de preço de casas. Se sua regressão linear foi treinada com casas até 4 quartos e no teste aparece uma de 8 você consegue extrapolar e prever de forma consistente. Com os modelos de árvore não.)\n",
    "* Em geral, não lidam bem com dados muito esparsos.\n",
    "* É menos interpretável que uma regressão linear ou uma árvore de decisão, por exemplo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliografia e Aprofundamento\n",
    "- [Problemas com as variáveis dummies](https://towardsdatascience.com/one-hot-encoding-is-making-your-tree-based-ensembles-worse-heres-why-d64b282b5769)\n",
    "- [When to avoid RF](https://stats.stackexchange.com/questions/112148/when-to-avoid-random-forest)\n",
    "- [Very skewed data](https://stats.stackexchange.com/questions/172842/best-practices-with-data-wrangling-before-running-random-forest-predictions)\n",
    "- [Como RF lida com missing](https://www.numpyninja.com/post/all-about-random-forests-and-handling-missing-values-in-them)\n",
    "- [Feature Importance x Feature Permutation](https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html#sphx-glr-auto-examples-ensemble-plot-forest-importances-py)\n",
    "- [Out-of-bag error](https://en.wikipedia.org/wiki/Out-of-bag_error)\n",
    "- [OOB error vs. test set error](https://uc-r.github.io/random_forests)\n",
    "- [Principais dúvidas no XGBoost](https://towardsdatascience.com/20-burning-xgboost-faqs-answered-to-use-the-library-like-a-pro-f8013b8df3e4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercício\n",
    "Utilize o dataset \"data/german_credit_data.csv\" para implementar o Random Forest e o XGBoost.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/german_credit_data.csv\", index_col=0)\n",
    "\n",
    "X = df.drop(columns=\"Risk\")\n",
    "y = df[\"Risk\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5ca4efd453f89b69dfc80cbb5ef222b3d24fc169b1efe916ea8f932329929361"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "259.188px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
