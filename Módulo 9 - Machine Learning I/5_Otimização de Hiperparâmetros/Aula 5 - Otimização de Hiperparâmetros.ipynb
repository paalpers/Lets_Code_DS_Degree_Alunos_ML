{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aula - Otimização de Hiperparâmetros\n",
    "\n",
    "Hoje, vamos discutir como selecionar os melhores hiperparâmetros para um dado problema.\n",
    "\n",
    "1. Introdução à otimização de hiperparâmetros.\n",
    "2. Usando Grid Search\n",
    "3. Usando Random Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introdução à otimização de hiperparâmetros\n",
    "\n",
    "Juntando o que foi visto no curso de estatística e o que foi visto até agora no curso de aprendizado de máquina, já aprendemos alguns modelos.\n",
    "\n",
    "<font size=4> <b> Classificação </b> </font>\n",
    "- Regressão Logística\n",
    "- Árvore de decisão (CART)\n",
    "- KNN\n",
    "\n",
    "\n",
    "<font size=4> <b> Regressão </b> </font>\n",
    "- Regressão Linear \n",
    "- Árvore de decisão (CART)\n",
    "- KNN\n",
    "\n",
    "\n",
    "Os de regressão logística e regressão linear não têm muitas escolhas prévias que se precisa fazer. No máximo determinar a melhor regularização.\n",
    "\n",
    "Já o KNN  e as árvores de decisão têm configurações intrínsecas que afetam completamente o resultado, como o número de vizinhos (\"K\") e a profundidade da árvore, respectivamente. Essas configurações a gente chama de __hiperparâmetros__, e eles controlam o aprendizado.\n",
    "\n",
    "Queremos então descobrir quais hiperparâmetros vão me ajudar a gerar o melhor modelo possível para o meu problema. Para um ou até dois parâmetros esse processo pode ser feito facilmente \"na mão\". Porém, à medida que a quantidade de hiperparâmetros aumenta, a quantidade de testes que temos que fazer aumentará __exponencialmente__ e será bem útil fazer uso de algumas técnicas conhecidas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Diferença entre hiperparâmetro e parâmetro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As nomenclaturas podem parecer um pouco confusas e parecidas a primeira vista, mas as diferenças são perceptíveis ao enxergar com mais atenção. \n",
    "\n",
    "O __parâmetro de um modelo é algo que será ajustado no processo de treinamento dele e depende dos dados__. Os parâmetros são parte do modelo e são aprendidos através dos dados. Geralmente os parâmetros são estimados utilizando-se algum algoritimo de otimização.\n",
    "\n",
    "Já um __hiperparâmetro é uma configuração externa que controla o processo de treinamento__. Os hiperparâmetros não são estimados diretamente pelos dados, como os parâmetros. Geralmente fazemos um tuning para estimá-los.\n",
    "\n",
    "| Parâmetro | Hiperparâmetro |\n",
    "|-----------|----------------|\n",
    "| Configurações internas do modelo | São explicitamente especificadas para controlar o treinamento |\n",
    "| Essenciais para realizar as predições | São essenciais para otimizar o modelo | \n",
    "| Especificadas ou estimadas DURANTE o treinamento | Definidas ANTES do treinamento |\n",
    "| São internas ao modelo | São externas ao modelo |\n",
    "| São aprendidas e setadas pelo modelo | Setadas manualmente via tuning |\n",
    "| Estimados por algoritimos de otimização como Gradiente Descendente | Estimados via tuning dos hiperparâmetros |\n",
    "| Decidem a performance em dados desconhecidos | Decidem a qualidade do modelo |\n",
    "| Exemplos: coeficientes da equação em uma Regressão Linear ou Logística, as regras criadas pela Árvore de Decisão, o centróide do cluster | Profundidade da árvore, o K do KNN |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Como otimizar hiperparâmetros?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A forma mais direta de pensar em como fazer essa otimização é assumir que cada escolha de hiperparâmetros é um modelo diferente. Assim, vamos treinar o modelo com cada escolha em um conjunto de treino, e comparar todos com uma estratégia de avaliação de modelos (usando um conjunto de validação ou uma validação cruzada).\n",
    "\n",
    "Isso é o mesmo que fizemos até agora para avaliação de modelos.\n",
    "\n",
    "A única diferença é que, como dito antes, a quantidade de escolhas cresce exponencialmente com a quantidade de hiperparâmetros. Se tivermos 2 hiperparâmetros, cada um com 4 valores, teríamos $4^2 = 16 $ escolhas possíveis. Se tivermos 4 hiperparâmetros, teríamos $ 4^4 = 256 $ escolhas possíveis.\n",
    "\n",
    "Isso nos motiva a criar estratégias quanto a como avaliar todas essas escolhas. Existem 2 estratégias básicas que usamos:\n",
    "- Grid Search\n",
    "- Random Search\n",
    "\n",
    "Vamos exemplificar cada uma dessas estratégias usando um modelo de árvore de decisão."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos começar sem usar nenhuma estratégia, e ver qual seria o nosso baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando as bibliotecas para matemática e visualização\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('../data/winequality.names', 'r') as fp:\n",
    "    print(fp.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pegando os dados\n",
    "wine = pd.read_csv('../data/wine_quality_white.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separa em treino e teste\n",
    "X = wine.drop(columns=['quality'])\n",
    "y = wine['quality']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, shuffle=True, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos converter nosso problema de multi-classes em um binário"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converte y_train para dataframe\n",
    "\n",
    "\n",
    "# Verifica classes desbalanceadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instancia o DT\n",
    "model_sem_otim = \n",
    "\n",
    "# Faz a validação cruzada\n",
    "results_no_optim = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_no_optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\" Acurácia treino: {100*results_no_optim['train_score'].mean():0.1f}\")\n",
    "\n",
    "acc_sem_otimizacao = results_no_optim['test_score'].mean()\n",
    "print(f\" Acurácia na validação: {100*acc_sem_otimizacao:0.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Faz o fit nos dados de treino\n",
    "\n",
    "\n",
    "# Faz a predição no teste\n",
    "\n",
    "\n",
    "print(f\" Acurácia teste: {100*accuracy_score(y_test, y_pred):0.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "É o nosso método extensivo e de força bruta. Escolhemos os valores que queremos testar para nossos hiperparâmetros e testamos todas as escolhas possíveis. Essa estratégia vai ser __MUITO__ custosa computacionalmente e tende a demorar bastante.\n",
    "\n",
    "<center><img src=\"https://www.researchgate.net/profile/Karl-Ezra-Pilario/publication/341691661/figure/fig2/AS:896464364507139@1590745168758/Comparison-between-a-grid-search-and-b-random-search-for-hyper-parameter-tuning-The.png\" style=\"height: 350px\"/></center>\n",
    "\n",
    "\n",
    "O `scikit-learn` tem um função que pode nos ajudar nesse processo. Está dentro da parte de `model_selection` e se chama [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV), que utiliza o método de validação cruzada.\n",
    "\n",
    "Atenção com a escolha da métrica:\n",
    "\n",
    "<img src=\"images/model_evaluation.png\">\n",
    "\n",
    "[link](https://scikit-learn.org/stable/modules/model_evaluation.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instancia novamente O DT\n",
    "model_grid = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Hiperparâmetros do DecisionTreeClassifier:\")\n",
    "model_grid.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos criar um grid de parâmetros a serem testados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Critério do split\n",
    "criterions = ['gini', 'entropy']\n",
    "\n",
    "# Profundidades máximas que iremos testar\n",
    "max_depth = list(np.arange(2, 10))\n",
    "max_depth.append(None)\n",
    "\n",
    "# Número de pontos mínimos necessário para permitir um split no nó\n",
    "min_samples_split = np.arange(4, 11)\n",
    "\n",
    "# Número de pontos mínimos que podem existir em cada folha (nó final)\n",
    "min_samples_leaf = np.arange(2, 5)\n",
    "\n",
    "# Criamos o grid de escolhas\n",
    "params_grid = {'criterion': criterions,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf}\n",
    "\n",
    "params_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total de modelos a serem comparados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(criterions)*len(max_depth)*len(min_samples_split)*len(min_samples_leaf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importa a classe GridSearchCV do sklearn.model_selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construindo o objeto \"otimizador via grid search com validação cruzada\" verbose=2\n",
    "grid_search = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Faz o fit do grid search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(grid_search.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converte cv_results_ em pandas dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retorna os melhores parâmetros e o melhor score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtém o best_score do treino\n",
    "acc_grid_search_train = df.sort_values(\"rank_test_score\")['mean_train_score'].iloc[0]\n",
    "print(f\" Acurácia treino: {100*acc_grid_search_train:0.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtém o best_score da validação\n",
    "acc_grid_search = \n",
    "\n",
    "print(f\" Acurácia validação: {100*acc_grid_search:0.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retorna o best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the best model in a variable to reference later\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No final da validação cruzada o grid search, por default, re-treina o modelo utilizando os melhores parâmetros encontrados no dataset inteiro. E retorna para a gente um método `predict` com esse modelo.\n",
    "\n",
    "For multiple metric evaluation, this needs to be a str denoting the scorer that would be used to find the best parameters for refitting the estimator at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Faz o predict no conjunto de teste\n",
    "\n",
    "\n",
    "print(f\" Acurácia teste: {100*accuracy_score(y_test, y_pred_grid):0.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Random Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Random Search](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html#sklearn.model_selection.RandomizedSearchCV)\n",
    "\n",
    "É parecido com o grid search, pois vamos montar um grupo de escolhas possíveis. Porém, ao invés de compararmos todas as escolhas, nós pegamos uma __amostra aleatória__ (sem reposição) delas, e selecionamos o melhor caso dentro dessa amostra. \n",
    "\n",
    "<center><img src=\"https://www.researchgate.net/profile/Karl-Ezra-Pilario/publication/341691661/figure/fig2/AS:896464364507139@1590745168758/Comparison-between-a-grid-search-and-b-random-search-for-hyper-parameter-tuning-The.png\" style=\"height: 350px\"/></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instancia DT novamente\n",
    "model_random = \n",
    "\n",
    "# Importa model_selection.RandomizedSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# O parâmetro n_iter vai controlar o tamanho da nossa amostra.\n",
    "random_search = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Faz o fit do random_search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converte o cv_results_ em pandas dataframe\n",
    "df = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retorna melhores parâmetros\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtém o best_score do treino\n",
    "acc_random_search_train = df.sort_values(\"rank_test_score\")['mean_train_score'].iloc[0]\n",
    "print(f\" Acurácia treino: {100*acc_random_search_train:0.1f}\")\n",
    "\n",
    "# Obtém o best_score da validação\n",
    "acc_random_search = random_search.best_score_\n",
    "print(f\" Acurácia validação: {100*acc_random_search:0.1f}\")\n",
    "\n",
    "# Faz o predict no X_test\n",
    "y_pred_random = \n",
    "\n",
    "print(f\" Acurácia teste: {100*accuracy_score(y_test, y_pred_random):0.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparando resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Comparação das Acurácias: \")\n",
    "print('Acurácia sem Otimização:         ', np.round(100*acc_sem_otimizacao, 2))\n",
    "print('Acurácia com GridSearchCV:       ', np.round(100*acc_grid_search, 2))\n",
    "print('Acurácia com RandomizedSearchCV: ', np.round(100*acc_random_search, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avaliando mais de uma métrica\n",
    "Podemos passar mais de uma métrica para avaliação, mas a escolha de melhor modelo e parâmetros irá depender da métrica indicada no parâmetro `refit`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Metrics for Evualation:\n",
    "met_grid= ['accuracy', 'f1']\n",
    "\n",
    "random_search_m = RandomizedSearchCV(estimator = model_random, \n",
    "                                   param_distributions = params_grid, \n",
    "                                   scoring=met_grid,\n",
    "                                   refit='accuracy',\n",
    "                                   n_iter=10, \n",
    "                                   cv=3, \n",
    "                                   verbose=2,\n",
    "                                   return_train_score=True)\n",
    "\n",
    "random_search_m.fit(X_train, y_train)\n",
    "y_pred = random_search_m.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(random_search_m.cv_results_)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____________________________\n",
    "_____________________________\n",
    "_____________________________\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercícios\n",
    "1. Faça a otimização de parâmetros para o Random Forest e o KNN das aulas anteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Utilize o exemplo abaixo para investigar o comportamento de alguns hiperparâmetros "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of integers 1 to 30\n",
    "# integers we want to try\n",
    "hiperparameter_range = range(1, 51)\n",
    "\n",
    "# list of scores from k_range\n",
    "train_scores = []\n",
    "pred_scores = []\n",
    "\n",
    "# 1. we will loop through reasonable values of hiperparameter\n",
    "for k in hiperparameter_range:\n",
    "    # 2. run KNeighborsClassifier with k neighbours\n",
    "    model = DecisionTreeClassifier(max_depth=k)\n",
    "    # 3. obtain cross_validate for KNeighborsClassifier with k neighbours\n",
    "    scores = cross_validate(model, X_train, y_train, cv=3, scoring='accuracy')\n",
    "    # 4. append mean of scores\n",
    "    train_scores.append(scores['test_score'].mean())\n",
    "    # 5. train model\n",
    "    model.fit(X_train, y_train)\n",
    "    # 6. predict on test\n",
    "    y_pred = model.predict(X_test)\n",
    "    # 7. append accuracy score for predictions\n",
    "    pred_scores.append(accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the value of hiperparameter (x-axis) versus the accuracy (y-axis)\n",
    "plt.plot(hiperparameter_range, train_scores, c='blue', label='Cross-Validated Accuracy')\n",
    "plt.plot(hiperparameter_range, pred_scores, c='red', label='Prediction Accuracy')\n",
    "\n",
    "plt.xlabel('Value of hiperparameter')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliografia e Aprofundamento\n",
    "- [Bayesian x Random Search](https://miro.medium.com/max/1400/1*Xfnh-biDrMCECEO37qecKQ.png)\n",
    "- [Outras otimizações: Evolutionary Search e Gradient Search](https://www.youtube.com/watch?v=TP9W7hmb0Bs)\n",
    "- [XGBoost Hyperparameter Tuning - A Visual Guide](https://kevinvecmanis.io/machine%20learning/hyperparameter%20tuning/dataviz/python/2019/05/11/XGBoost-Tuning-Visual-Guide.html)\n",
    "- [Como o tunning afeta o overfiting - bem legal!](https://github.com/tirthajyoti/Machine-Learning-with-Python/blob/master/Complexity_Learning_curves/Complexity_Learning_Analysis_Lending_Data.ipynb)\n",
    "- [Plot de hiperparâmetros](https://www.ritchieng.com/machine-learning-efficiently-search-tuning-param/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cb5f626699f206ef97176a4f092b8d9f6e52ae1f84b4bb3163daf9eb25ca3519"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('aula_venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
