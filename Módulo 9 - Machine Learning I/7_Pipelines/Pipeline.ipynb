{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline\n",
    "\n",
    "#### Tópicos da aula\n",
    "- Transformers x Estimators\n",
    "- Pipeline\n",
    "- FunctionTransform\n",
    "- ColumnTransform\n",
    "\n",
    "___________________________\n",
    "\n",
    "\n",
    "[Pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) é uma classe do sklearn que permite aplicar uma sequência de transformações em um estimador final. <br>\n",
    "Para isso, os passos intermediários devem ter implementados métodos de `fit` e `transform` enquanto o estimador final só precisa ter o `fit` implementado. <br>\n",
    "O propósito do `pipeline` é:\n",
    "- reunir várias etapas para serem validadas de forma cruzada (cross-validation) ao definir parâmetros diferentes\n",
    "- ajudar a criar códigos que possuam um padrão que possa ser facilmente entendido e compartilhando entre times de cientista e engenheiro de dados.\n",
    "\n",
    "<img src=\"images/pipeline.png\" text=\"https://nbviewer.org/github/rasbt/python-machine-learning-book/blob/master/code/ch06/ch06.ipynb#Combining-transformers-and-estimators-in-a-pipeline\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- __Transformer:__ Um transformador se refere à um objeto de uma classe que possuim os métodos fit() e transform() e que nos ajudam a transformar o dado na forma que queremos. OneHotEncoder, SimpleImputer e MinMaxScaler são exemplos de transformers.\n",
    "- __Estimator:__ Um estimador se refere à um algoritmo de ML. Ele é um objeto de uma classe que possui os métodos fit() e predict(). [Aqui](https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html) se encontram exemplos de estimadores.\n",
    "\n",
    "Hoje vamos utilizar um dataset mais simples de exemplo. Usaremos os dados de gorjeta cuja descrição encontra-se [nesse link](https://vincentarelbundock.github.io/Rdatasets/doc/reshape2/tips.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos importar o dataset\n",
    "df = sns.load_dataset('tips')\n",
    "\n",
    "# Add missing values -> pra dar uma graça no dataset\n",
    "df.iloc[[1, 2, 4, 12], [2]] = np.nan\n",
    "df.iloc[[10, 20, 40, 120, 222], [1]] = np.nan\n",
    "df.iloc[[61, 27, 145, 212], [3]] = np.nan\n",
    "df.iloc[[143, 237, 48, 102, 20], [4]] = np.nan\n",
    "df.iloc[[71, 172, 194, 182], [5]] = np.nan\n",
    "df.iloc[[83, 90, 33, 228], [6]] = np.nan\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos ver as dimensões dele\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observe que cada feature tem pelo menos 1 nulo (afinal, non-null delas é menor que 150)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirmando quantos nulos temos em cada coluna\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para as minhas features numéricas, eu quero seguir os seguintes passos de pré-processamento,\n",
    "1. \"padronizar\" as minhas features (ou \"normalizar\", deixar elas com média 0 e desvio padrão 1),\n",
    "2. adicionar a mediana em qualquer valor nulo,\n",
    "3. treinar o algoritmo de ML.\n",
    "\n",
    "Neste caso, note que eu preciso \"treinar\" os passos (1), (2) e (3) todos na base de treino, e depois só aplicar eles na base de validação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# particionando os dados\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.drop(columns=['total_bill']), \n",
    "                                                    df['total_bill'], \n",
    "                                                    test_size=.2, \n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para os passos de processamento das features, faremos todos juntos, com um Pipeline.\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como utilizar: <br>\n",
    "O Pipeline é construído com uma lista de pares (key, value) nos quais a key é uma string que contém um nome para o step escolhido e o valor é o objeto da classe:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/pipeline_sintaxe.png\" width=500 />\n",
    "\n",
    "tal que `transformacao_1` é o nome que você quer dar para essa etapa e `transformacao_1()` é a classe instanciada da transformação.\n",
    "A lista com as transformações deve ser passada já com a sequencia em que elas devem ser aplicadas.\n",
    "<br>\n",
    "O Pipeline segue o mesmo framework do sklearn e por isso temos os métodos `.fit()`, `fit_transform()` e `.transform()` para os transformes e `.fit()` e `predict()` quando temos estimadores definidos dentro da sequência do pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separa variáveis numéricas das categóricas\n",
    "num_cols = X_train.select_dtypes(\"number\").columns\n",
    "cat_cols = X_train.select_dtypes(exclude=\"number\").columns\n",
    "\n",
    "# Cria nosso Pipeline com SimpleImputer, StandardScaler e KNeighborsRegressor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os passos do pipeline podem ser acessados pelos índices ou passando a key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# por índice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# por key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Podemos visualizar nosso pipe\n",
    "from sklearn import set_config\n",
    "set_config(display=\"diagram\")\n",
    "pipe_knn  # click on the diagram below to see the details of each step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos rodar nosso pipeline no treino utilizando apenas as colunas numéricas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fazendo o predict direto:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agora avaliamos o modelo no nosso conjunto de validação.\n",
    "print(f'MSE: {mean_squared_error(y_test, y_pred):.1f}')\n",
    "print(f'MAE: {mean_absolute_error(y_test, y_pred):.1f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tudo junto\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "\n",
    "# particionando os dados\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df.drop(columns=['total_bill']), \n",
    "    df['total_bill'], \n",
    "    test_size=.2, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Define variáveis numéricas e categóricas\n",
    "num_cols = X_train.select_dtypes(\"number\").columns\n",
    "cat_cols = X_train.select_dtypes(exclude=\"number\").columns\n",
    "\n",
    "# Cria nosso Pipeline com SimpleImputer, StandardScaler e KNeighborsRegressor\n",
    "pipe_knn = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', KNeighborsRegressor(n_neighbors=7))],\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Vamos rodar nosso pipeline no treino\n",
    "pipe_knn.fit(X_train[num_cols], y_train)\n",
    "\n",
    "# Prediz no teste\n",
    "y_pred = pipe_knn.predict(X_test[num_cols])\n",
    "\n",
    "print()\n",
    "print(y_pred[:10])\n",
    "print()\n",
    "\n",
    "# Agora avaliamos o modelo no nosso conjunto de validação.\n",
    "print(f'MSE: {mean_squared_error(y_test, y_pred):.1f}')\n",
    "print(f'MAE: {mean_absolute_error(y_test, y_pred):.1f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por que isso é poderoso?\n",
    "\n",
    "Imagina se quiséssemos fazer validação cruzada. Nesse caso, em cada fold que escolhermos como validação, teriamos que refazer cada passo na base de treino da vez,\n",
    "1. achar a mediana daquela base de treino, e preencher os nulos,\n",
    "2. achar a média e a variância daquela base de treino, para padronizar as features,\n",
    "3. aí sim, treinamos o modelo na base de treino, e medimos a qualidade no fold de validação da vez.\n",
    "\n",
    "Mas usando Pipeline, a gente não precisa fazer tudo passo a passo. O Pipeline se encarrega de fazer tudo de uma vez para nós. É como se o nosso \"modelo\" agora fosse o pipeline completo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tunando hiperparâmetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos checar quais os parâmetros que podemos utilizar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tunando hiperparâmetros com 3-fold cross-validation e pipelines\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agora podemos acessar os atributos do grid\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E se eu tiver funções próprias ou queira aplicar alguma pronta do python?\n",
    "\n",
    "## [FunctionTransformer](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.FunctionTransformer.html)\n",
    "Com a `FunctionTransformer` conseguimos adicionar funções que não possuem os métodos `.fit()` e `.transform()` ao pipeline. A função criada deve retornar um pandas Dataframe ou um array do numpy a fim de podermos utilizá-lo com o Pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "# Para funções próprias\n",
    "def somar_10(df):\n",
    "    return df+10\n",
    "\n",
    "def quebra_coluna(df):\n",
    "    df['tip2'] = np.where(df['tip']<5, 0, 1)\n",
    "    return df\n",
    "\n",
    "def return_selected_cols(dataset, columns):\n",
    "    return dataset[columns]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E como adicionar tratamento nas variáveis categóricas?\n",
    "\n",
    "## [ColumnTransformer()](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html?highlight=columntransformer#sklearn.compose.ColumnTransformer)\n",
    "Essa classe serve para __especificarmos em quais colunas a transformação deve ser aplicada__. Seu uso é bem simples, deve-se nomear o tratamento, especificar qual ele deve ser e especificar as colunas nas quais ele deve ser aplicado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import KFold, GridSearchCV, train_test_split\n",
    "from sklearn.preprocessing import FunctionTransformer, OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "# Separa treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.drop(columns=['total_bill']), \n",
    "                                                    df['total_bill'], \n",
    "                                                    test_size=.2, \n",
    "                                                    random_state=42)\n",
    "\n",
    "# Define colunas categóricas\n",
    "cat_cols = list(X_train.select_dtypes('category').columns)\n",
    "print(f\"Colunas categoricas: {cat_cols}\")\n",
    "\n",
    "# Define colunas numéricas\n",
    "num_cols = list(X_train.select_dtypes('number').columns)\n",
    "print(f\"Colunas numéricas: {num_cols}\")\n",
    "\n",
    "# Para funções próprias\n",
    "def somar_10(df):\n",
    "    return df+10\n",
    "\n",
    "soma_10 = FunctionTransformer(somar_10, validate=False)\n",
    "\n",
    "# Define pipeline numérico\n",
    "num_pipe = Pipeline([\n",
    "    ('imputer_cv', SimpleImputer(strategy='median')),\n",
    "    ('log', FunctionTransformer(np.log)),\n",
    "    ('soma10', soma_10),\n",
    "    ('scaler_cv', StandardScaler()),\n",
    "], verbose=True)\n",
    "\n",
    "# Define pipeline categórico com SimpleImputer e OneHotEncoder\n",
    "cat_pipe = Pipeline([\n",
    "    ('imputer_cv', SimpleImputer(strategy='most_frequent')),\n",
    "    ('ohe', OneHotEncoder(sparse=False, handle_unknown='ignore', drop='first')),\n",
    "])\n",
    "\n",
    "# Concatena pipelines categóricos e numéricos com suas respectivas colunas\n",
    "\n",
    "\n",
    "\n",
    "# Define pipeline final com o preprocessor e o estimador\n",
    "\n",
    "\n",
    "\n",
    "# Tunando hiperparâmetros com 3-fold cross-validation e pipelines\n",
    "parameters = {'model__n_neighbors': [3, 4, 5],\n",
    "              'model__p': [1,2],\n",
    "              'model__weights': [\"uniform\", \"distance\"]}\n",
    "\n",
    "kfold = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "grid = GridSearchCV(pipeline, param_grid=parameters, cv=kfold, n_jobs=-1, return_train_score=True, scoring=\"neg_mean_squared_error\")\n",
    "\n",
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliografia e Aprofundamento\n",
    "- [Python Machine Learning Book](https://github.com/rasbt/python-machine-learning-book-3rd-edition)\n",
    "- [Documentação](https://scikit-learn.org/stable/modules/compose.html)\n",
    "- [ColumnTransformer](https://scikit-learn.org/stable/modules/compose.html#columntransformer-for-heterogeneous-data)\n",
    "- [FeatureUnion](https://scikit-learn.org/stable/modules/compose.html#featureunion-composite-feature-spaces)\n",
    "- https://medium.com/data-hackers/como-usar-pipelines-no-scikit-learn-1398a4cc6ae9\n",
    "- [Pipelines e funções próprias: FuncionTransformer](https://towardsdatascience.com/using-functiontransformer-and-pipeline-in-sklearn-to-predict-chardonnay-ratings-9b13fdd6c6fd)\n",
    "- [Custom Functions: Como criar classes e usá-las no pipeline](https://tiaplagata.medium.com/how-scikit-learn-pipelines-make-your-life-so-much-easier-3cfbfa1d9da6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5ca4efd453f89b69dfc80cbb5ef222b3d24fc169b1efe916ea8f932329929361"
  },
  "kernelspec": {
   "display_name": "Python (DataScience)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
