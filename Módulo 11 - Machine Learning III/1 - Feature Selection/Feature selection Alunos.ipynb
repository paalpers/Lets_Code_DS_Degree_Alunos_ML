{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aula - Feature Selection\n",
    "\n",
    "Nas próximas duas aulas vamos explorar os seguintes tópicos em Python:\n",
    "\n",
    "- 1) Introdução\n",
    "- 2) Métodos de seleção de features supervisonado\n",
    "- 3) Métodos de filtro\n",
    "- 4) Métodos wrapper\n",
    "- 5) Métodos híbridos\n",
    "- 6) Médotos embutidos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-11T22:18:45.719993Z",
     "start_time": "2022-03-11T22:18:35.104865Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy  as np\n",
    "import pandas as pd \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-11T22:18:45.751977Z",
     "start_time": "2022-03-11T22:18:45.725992Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, ConfusionMatrixDisplay\n",
    "\n",
    "def metricas_classificacao(estimator, X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    # ============================================\n",
    "\n",
    "    print(\"\\nMétricas de avaliação de treino:\")\n",
    "\n",
    "    y_pred_train = estimator.predict(X_train)\n",
    "\n",
    "    ConfusionMatrixDisplay.from_predictions(y_train, y_pred_train)\n",
    "    plt.show()\n",
    "\n",
    "    print(classification_report(y_train, y_pred_train))\n",
    "\n",
    "    # ============================================\n",
    "\n",
    "    print(\"\\nMétricas de avaliação de teste:\")\n",
    "\n",
    "    y_pred_test = estimator.predict(X_test)\n",
    "\n",
    "    ConfusionMatrixDisplay.from_predictions(y_test, y_pred_test)\n",
    "    plt.show()\n",
    "\n",
    "    print(classification_report(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos utilizar o dataset Breast Cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "#\n",
    "# Load do Dataset Breast Cancer\n",
    "bc = datasets.load_breast_cancer(as_frame=True)\n",
    "X = bc.data\n",
    "y = bc.target\n",
    "\n",
    "# Cria training and test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "____\n",
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Introdução\n",
    "\n",
    "O processo de **feature selection** (**seleção de atributos**) consiste na escolha, com base em alguns critérios, de um **subconjunto do conjunto original** de features de um dado problema que proporcionem um modelo com performance comparável ao modelo treinado com todas as features. \n",
    "\n",
    "<img src=https://miro.medium.com/max/694/0*D_jQ5yBsvCZjEYIW width=400>\n",
    "\n",
    "O resultado do processo de feature selection é uma **redução na dimensionalidade** do espaço de features do problema (mas aqui, diferente do PCA, trabalhamos no espaço de features originais!)\n",
    "\n",
    "Assim, o processo remove features redundantes ou irrelevantes. \n",
    "\n",
    "Dentre as vantagens do procedimento, podemos destacar:\n",
    "\n",
    "- Maior eficiência no treinamento (afinal, reduzimos a quantidade de informação a ser processada);\n",
    "- Eliminação de redundâncias (como multicolinearidade, por exemplo, que pode ser problemática para alguns estimadores);\n",
    "- Um modelo com menos features é, em geral, mais facilmente interpretável;\n",
    "- Ao reduzirmos o número de features, a complexidade da hipótese é reduzida, o que pode favorecer a generalização e melhorar a predição do estimador;\n",
    "\n",
    "\n",
    "O princípio da [navalha de Occam](https://pt.wikipedia.org/wiki/Navalha_de_Ockham) é relevante no contexto de feature selection em projetos de machine learning. Sugiro [este post](https://machinelearningmastery.com/ensemble-learning-and-occams-razor/#:~:text=Occam's%20razor%20suggests%20that%20in,narrow%20and%20not%20generalize%20well.) para uma discussão deste princípio como uma heurística para a construção de modelos. Para uma discussão mais profunda, sugiro [este paper](https://www.aaai.org/Papers/KDD/1998/KDD98-006.pdf).\n",
    "\n",
    "Alguns modelos que são sensíveis à atributos irrelevantes:\n",
    " - Regressão Linear e Logística (principalmente se forem correlacionados)\n",
    " - KNN\n",
    " - SVM\n",
    " - Redes Neurais\n",
    "\n",
    "## Etapas do pré-processamento dos dados\n",
    "\n",
    "<img src=\"images/pre-processing_order.png\" width=700>\n",
    "\n",
    "## Tipos de métodos de feature selection\n",
    "Assim como temos modelos supervisionados e não supervisionados, teremos técnicas de seleção de feature que dependem da variável target ou não:\n",
    "\n",
    "<img src='https://www.kdnuggets.com/wp-content/uploads/Fig1-Butvinik-feature-selection-overview.jpg' text='https://www.kdnuggets.com/2021/06/feature-selection-overview.html'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Técnicas Supervisionadas\n",
    "Nesta aula, estudaremos alguns métodos de seleção de features que utilizam o target.\n",
    "\n",
    "\n",
    "Podemos classificar os métodos de seleção de features supervisionados de acordo com a sua interação com o modelo de aprendizado: \n",
    "\n",
    "<img src=\"https://www.kdnuggets.com/wp-content/uploads/Fig3-Butvinik-feature-selection-overview.jpg\" text=\"https://www.kdnuggets.com/2021/06/feature-selection-overview.html\"/>\n",
    "\n",
    "\n",
    "### 1. Filtro\n",
    "\n",
    "Aqui, utilizamos **técnicas estatísticas** como ganho de informação, teste qui-quadrado, pontuação de Fisher e correlação com o target. As features são rankeadas de acordo com a técnica estatística escolhida e as melhores são selecionadas. Esse método **independe do estimador escolhido** e necessita de menos tempo computacional. A principal vantagem é que podemos trocar o estimador que as principais features serão as mesmas, não necessitando refazer essa etapa.\n",
    "\n",
    "A técnica estatística a ser escolhida dependerá do tipo da sua variável dependente, se o target é categórico ou contínuo, e dos tipos da suas variáveis independentes, se suas features são categóricas ou contínuas.\n",
    "\n",
    "<img src=\"images/How-to-Choose-Feature-Selection-Methods.png\" text='imagem modificada de: machinelearningmastery.com/feature-selection-with-real-and-categorical-data/' width=600px>\n",
    "\n",
    "Vamos agora ver um exemplo de filtro utilizando o \n",
    "[SelectKBest](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html#sklearn.feature_selection.SelectKBest) do sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para o nosso caso, qual(is) desses filtros podemos utilizar?\n",
    "\n",
    "[ANOVA (**An**alysis **o**f **Va**riance)](https://towardsdatascience.com/statistics-in-python-using-anova-for-feature-selection-b4dc876ef4f0) pressupõe que você tem variáveis contínuas de um lado e categóricas do outro. Para avaliar as features mais importantes, ela compara os grupos categóricos analisando se há uma igual variância nos dados contínuos. Se para os diferentes grupos temos uma mesma variância, isso significa que essa feature contínua não é relevante para separar os grupos e, portanto, pode ser eliminada da modelagem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importa SelectKBest e f_classif\n",
    "\n",
    "\n",
    "# instancia SelectKBest\n",
    "fs =\n",
    "\n",
    "# Cria novo df com fit_transform\n",
    "X_new = \n",
    "\n",
    "print(f'Quantidade de features antes: {X_train.shape[1]}, quantidade de features depois {X_new.shape[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_feature_names_out retorna o nome das features selecionadas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores_ retorna o score de cada uma das features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in zip(X_train.columns, fs.scores_):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **informação mútua (MI)** entre duas variáveis ​​aleatórias é um valor não negativo, que mede a dependência entre as variáveis. É igual a zero se e somente se duas variáveis ​​aleatórias são independentes, e valores mais altos significam maior dependência.\n",
    "\n",
    "Utilize a classe [mutual_info_classif](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_classif.html#sklearn.feature_selection.mutual_info_classif) para criar um novo dataframe contendo apenas as 10 features mais relevantes segundo essa abordagem e print o nome dessas features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Wrapper\n",
    "\n",
    "Nesse método, diferentes combinações de features são selecionadas, avaliadas utilizando-se um modelo e comparadas com as outras combinações com base dos resultados desse estimador. Dessa forma, a escolha das features depende do estimador escolhido e a **busca será feita em todas as possíveis combinações de features utilizando a métrica escolhida**.\n",
    "\n",
    "As estratégias mais conhecidos são: <br>\n",
    "* Forward selection - [SequentialFeatureSelector](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SequentialFeatureSelector.html) <br>\n",
    "* Backward elimination - [SequentialFeatureSelector](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SequentialFeatureSelector.html), [RFE](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html#sklearn.feature_selection.RFE) <br>\n",
    "* Stepwise selection (Bi-directional elimination) - [SequentialFeatureSelector do mlxtend](http://rasbt.github.io/mlxtend/api_subpackages/mlxtend.feature_selection/#sequentialfeatureselector)<br>\n",
    "* Permutation Importance - [permutation_importance](https://scikit-learn.org/stable/modules/generated/sklearn.inspection.permutation_importance.html)\n",
    "<br><br>\n",
    "\n",
    "__Foward Selection__\n",
    "\n",
    "Etapas:\n",
    "\n",
    "1) Começa com um modelo que não contém variáveis (chamado de Null Model)\n",
    "2) Faz um modelo com cada uma das features separadamente\n",
    "3) Escolhe a feature mais significativa\n",
    "4) Roda modelos com a feature selecionada e adicionando mais uma\n",
    "5) Escolhe o melhor modelo\n",
    "6) Repete processo 4 e 5 até acabarem as features\n",
    "\n",
    "<img src=\"https://quantifyinghealth.com/wp-content/uploads/2019/10/forward-stepwise-algorithm.png\" text=\"https://quantifyinghealth.com/stepwise-selection/\" width=400>\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "__Backward Selection__\n",
    "\n",
    "Etapas:\n",
    "\n",
    "1) Roda um modelo com todas as features\n",
    "2) Escolhe a feature menos significativa e elimina ela\n",
    "3) Roda um modelo com as features restantes\n",
    "4) Repete o processo 2 e 3 sequencialmente\n",
    "\n",
    "A vantagem do Backward Selection é considerar a interação entre as features antes de eliminá-las, mas se o número de features for muito grande a seleção pode demorar demais.\n",
    "\n",
    "<br>\n",
    "\n",
    "__Bi-directional Elimination__\n",
    "\n",
    "Muito semelhante ao Foward selection, mas ao adicionar uma nova variável ele verifica a importantância de todas as features e se encontrar alguma com significância menor que a determinada previamente, remove essa feature específica por meio do Backward Elimination.\n",
    "\n",
    "Portanto, é uma combinação de seleção para frente e eliminação para trás.\n",
    "\n",
    "Etapas:\n",
    "\n",
    "1) Execute a próxima etapa do Foward Selection.\n",
    "\n",
    "2) Execute todas as etapas de eliminação para trás. Ou seja, qualquer recurso adicionado anteriormente com p-value > significancia será removido do modelo).\n",
    "\n",
    "3) Repita as etapas 2 e 3 até obtermos um conjunto final ótimo de recursos.\n",
    "\n",
    "\n",
    "Para saber mais acesse o [link 1](https://quantifyinghealth.com/stepwise-selection/) e [link 2](https://www.analyticsvidhya.com/blog/2020/10/a-comprehensive-guide-to-feature-selection-using-wrapper-methods-in-python/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RFE\n",
    "\n",
    "Conheceremos agora o método **Recursive Feature Elimination** (RFE).\n",
    "\n",
    "O RFE é um método que se utiliza de um estimador capaz de atribuir um score de **importância** a cada uma das features.\n",
    "\n",
    "> Por exemplo, podemos olhar para os coeficientes de um modelo linear (`coef_`), ou então, para os scores de importância de features (`feature_importances_`). Esse método só irá funcionar se o estimador escolhido retorna `coef_` ou `feature_importances_`.\n",
    "\n",
    "O método então considera recursivamente **subconjuntos cada vez menores de features**, da seguinte maneira:\n",
    "\n",
    "- O estimador é treinado inicialmente com todas as features;\n",
    "- A importância de cada uma das features é calculada;\n",
    "- **As features menos importantes são retiradas do conjunto de features**;\n",
    "- O processo recomeça, até que o número  desejado de features seja alcançado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sendo assim, temos dois hiperparâmetros importantes na classe [sklearn.feature_selection.RFE](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html):\n",
    "\n",
    "- `estimator`: o estimador que irá disponibilizar os scores de importância de features;\n",
    "- `n_features_to_select`: a quantidade de features que o subconjunto final terá.\n",
    "\n",
    "Na prática, podemos utilizar um gridsearch para otimizar estes dois hiperparâmetros, ou então utilizar a classe [RFECV](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFECV.html), que determina o melhor número de features automaticamente.\n",
    "\n",
    "Vamos ver o método na prática!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-11T22:25:13.156721Z",
     "start_time": "2022-03-11T22:25:12.752801Z"
    }
   },
   "outputs": [],
   "source": [
    "# Vamos usar como estimador uma DT e escolher as 10 features mais importantes\n",
    "# importa classes do DT e RFE\n",
    "\n",
    "\n",
    "# instancia e faz o fit com RFE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-11T22:25:13.487434Z",
     "start_time": "2022-03-11T22:25:13.467443Z"
    }
   },
   "outputs": [],
   "source": [
    "# support_ retorna uma máscara com as features selecionadas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ranking_ retorna a posição em que as features foram selecionadas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos ver como utilizar o [RFECV](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFECV.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-11T22:35:10.780784Z",
     "start_time": "2022-03-11T22:35:09.602376Z"
    }
   },
   "outputs": [],
   "source": [
    "# Importa classes do RFECV e StratifiedKFold\n",
    "\n",
    "\n",
    "# instancia StratifiedKFold\n",
    "\n",
    "\n",
    "# instancia e faz o fit com RFE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assim como na classe do cross validate, ele retorna um dicionário com os scores de cada split no atributo `cv_results_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-11T22:35:59.485218Z",
     "start_time": "2022-03-11T22:35:59.432247Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-11T22:36:29.480197Z",
     "start_time": "2022-03-11T22:36:29.460207Z"
    }
   },
   "outputs": [],
   "source": [
    "# Para saber o número de features escolhidos via CV usamos o n_features_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Também podemos ver graficamente\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na prática, podemos incluir o RFE como um passo da Pipeline e otimizar seus parâmetros com o grid search!\n",
    "\n",
    "Tentem fazer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-11T22:45:50.528916Z",
     "start_time": "2022-03-11T22:45:50.493936Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-10T01:09:17.336189Z",
     "start_time": "2022-03-10T01:07:56.122167Z"
    }
   },
   "outputs": [],
   "source": [
    "# Instancia RFE e Decision Tree\n",
    "\n",
    "\n",
    "# Cria pipeline\n",
    "\n",
    "\n",
    "# Define param_grid\n",
    "param_grid_ab = {\"rfe__n_features_to_select\" : range(1, X_train.shape[1]+1),\n",
    "                 \"rfe__estimator__max_depth\" : [5, 10, 50],\n",
    "                 \"model__max_depth\" : [10, 25, 50]\n",
    "                }\n",
    "\n",
    "# Instancia StratifiedKFold\n",
    "\n",
    "\n",
    "# Instancia RandomizedSearchCV\n",
    "\n",
    "\n",
    "# Fit do RandomizedSearchCV\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Percebam que utilizamos o mesmo modelo para escolher as melhores features (DT) e para predizer nosso target. Não é obrigatório utilizarmos o mesmo, mas a escolha de features estará otimizada para o modelo usado no método wrapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-10T01:09:36.708956Z",
     "start_time": "2022-03-10T01:09:36.701961Z"
    }
   },
   "outputs": [],
   "source": [
    "# retorna melhores parâmetros\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-10T01:10:03.346735Z",
     "start_time": "2022-03-10T01:10:02.403565Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Printa métricas de classificação\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos rodar o modelo sem RFE para comparar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-11T22:46:09.822022Z",
     "start_time": "2022-03-11T22:45:54.151861Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Permutation importance\n",
    "\n",
    "Neste método, utilizamos a função [`sklearn.inspection.permutation_importance()`](https://scikit-learn.org/stable/modules/generated/sklearn.inspection.permutation_importance.html), que vai criar permutações das features, mantendo um registro do score. O permutation_importance é definido como a **diminuição no score de um modelo quando uma única feature é embaralhada aleatoriamente. Este procedimento quebra a relação entre a feature e o target e utiliza a queda na pontuação do modelo como um indicativo de quanto o modelo depende dessa feature**.\n",
    "\n",
    "Por realizar diversas permutações, este método é mais custoso, mas tem a vantagem de eliminar o viés que features de alta cardinalidade carregam com o método baseado em impureza.\n",
    "\n",
    "Para maiores detalhes sobre o método, [clique aqui!](https://scikit-learn.org/stable/modules/permutation_importance.html#permutation-importance)\n",
    "\n",
    "> Observação: este é um método que pode ser usado com qualquer estimador!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-11T22:19:02.771114Z",
     "start_time": "2022-03-11T22:18:54.187540Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Faz o fit do DT\n",
    "\n",
    "\n",
    "# Calcula o permutation_importance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-11T22:19:03.252918Z",
     "start_time": "2022-03-11T22:19:02.976994Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Cria df a partir dos atributos\n",
    "df_perm = \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-11T22:19:03.847510Z",
     "start_time": "2022-03-11T22:19:03.256915Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 7))\n",
    "plt.title(\"Feature importances using permutation importance\")\n",
    "plt.barh(df_perm['features'], df_perm[\"importance\"], xerr=df_perm[\"std\"])\n",
    "plt.xlabel(\"score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______________________________________________________________________________________\n",
    "<br>\n",
    "\n",
    "### 3. Metodologia Híbrida\n",
    "\n",
    "Nessa metodologia o intuito é definir as principais features com um método de filtro e depois aplicar o método de wrapper para reduzir o espaço de features. Dessa forma, reduzimos a quantidade de testes a serem realizados por este último.\n",
    "<br><br><br>\n",
    "_______________________________________________________________________________________\n",
    "<br>\n",
    "\n",
    "### 4. Metodologia Embutida/Intrínseca (Embedded/Intrinsic)\n",
    "\n",
    "Nessa metodologia o próprio estimador possui uma forma de seleção de features. Os principais exemplos são métodos de árvore e regressão Lasso. \n",
    "No primeiro, a árvore seleciona uma feature em cada divisão, deixando por último as features menos relevantes. No segundo, as features menos relevantes têm o seu coeficiente zerado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LASSO\n",
    "\n",
    "Já conhecemos um método capaz de realizar feature selection: a **regularização L1 (LASSO)**.\n",
    "\n",
    "Diferente da regularização L2, quando utilizamos regularização L1 é possível zerar alguns dos parâmetros do modelo:\n",
    "\n",
    "<img src=https://ugc.futurelearn.com/uploads/assets/2b/fe/2bfe399e-503e-4eae-9138-a3d7da738713.png width=800>\n",
    "\n",
    "Embora ambas as modalidades de regularização tenham sido introduzidas com o intuito de simplificar o espaço de hipóteses, o LASSO faz isso de maneira explícita, efetivamente possibilitando a realização de feature selection!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No entanto, há um problema: são poucos os métodos que têm o LASSO incorporado (ex.: regressão linear, logística, XGBoost).\n",
    "\n",
    "Assim, se quisermos realizar feature selection utilizando outros estimadores, precisamos de técnicas mais genéricas, que foi o que vimos.\n",
    "\n",
    "Para utilizarmos o L1, uma abordagem possível é:\n",
    "\n",
    "- **treinar inicialmente um modelo com LASSO**; \n",
    "- identificar quais features **ainda estão presentes no modelo** (isto é, com `coef_` não nulo);\n",
    "- utilizar apenas estas features para treinar o estimador desejado.\n",
    "\n",
    "Vamos ver como o Lasso se comporta em um dataset conhecido?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale das features é obrigatório já que faremos uma regressão linear\n",
    "sc = MinMaxScaler()\n",
    "X_train_sc = sc.fit_transform(X_train)\n",
    "X_test_sc = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos criar um loop que calculará o valor dos coeficientes e a métrica de erro conforme aumentamos o valor da regularização $\\lambda$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "alphas = []\n",
    "coefs = []\n",
    "test_scores = []\n",
    "train_scores = []\n",
    "\n",
    "for alpha in np.arange(0, 0.002, 0.00001):\n",
    "    # Cria uma instância do Lasso Regression\n",
    "    lasso = Lasso(alpha=alpha)\n",
    "    \n",
    "    # Fit Lasso model\n",
    "    lasso.fit(X_train_sc, y_train)\n",
    "    \n",
    "    # Salva os scores do modelo (nesse caso é o coeficiente de determinação R²)\n",
    "    train_scores.append(lasso.score(X_train_sc, y_train))\n",
    "    test_scores.append(lasso.score(X_test_sc, y_test))\n",
    "\n",
    "    # Salva o valor de alpha usado\n",
    "    alphas.append(alpha)\n",
    "\n",
    "    # Salva o valor dos coeficientes estimados\n",
    "    coefs.append(lasso.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatena os valores de alpha e dos coeficientes em uma única lista\n",
    "concat_data = [np.append(alphas[i], coefs[i]) for i in range(len(alphas))]\n",
    "\n",
    "# Cria dataframe com um valor de lambda por linha e as colunas como valores dos coeficientes\n",
    "df = pd.DataFrame(concat_data, columns=['lambda']+bc.feature_names.tolist())\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui já podemos ver que alguns coeficientes foram zerados pela regularização. Vamos olhar o heatmap de correlação entre as features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.clustermap(df.drop('lambda', axis=1).corr(), method=\"complete\", cmap='RdBu', annot=True, \n",
    "               annot_kws={\"size\": 7}, vmin=-1, vmax=1, figsize=(15,12));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Temos muitas variáveis independentes correlacionadas e já sabemos que isso impacta no nosso modelo de regressão linear. Lasso escolhe aleatóriamente uma das variáveis multicolineares e zera as demais. Isso pode impactar na interpretabilidade do nosso modelo.\n",
    "\n",
    "Vamos ver como fica o coeficiente de determinação tanto para o treino quanto para o teste conforme aumentamos nossa regularização $\\lambda$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "sns.lineplot(x=alphas, y=train_scores, label='Train')\n",
    "sns.lineplot(x=alphas, y=test_scores, label='Teste')\n",
    "plt.ylabel('Coeficiente de determinação (R²)')\n",
    "plt.xlabel('Lambda')\n",
    "plt.title('Qual o impacto da regularização Lasso no erro de predição do treino e do teste?');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conforme esperado, o R² do treino diminui com o aumento da regularização enquanto o do teste atinge um plato próximo de 0.75 e depois começa a cair. Nosso valor ideal de regularização está próximo de $\\lambda = 0.00015$.\n",
    "\n",
    "Vamos ver agora como o aumento da regularização afeta os coeficientes das nossa features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_lim = [-0.00001, 0.0012]\n",
    "figure_size = (15,7)\n",
    "df.plot(x='lambda', figsize=figure_size)\n",
    "plt.axvline(x=0.00015, linestyle='--')\n",
    "plt.xlim(x_lim)\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))\n",
    "plt.ylabel('Coeficientes encontrados')\n",
    "plt.xlabel('Lambda')\n",
    "plt.title('Qual o impacto da regularização Lasso nos coeficientes?', size=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Também podemos ver quantas features sobram conforme aumentamos a regularização:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qtd_features_zeradas = df.groupby('lambda').agg(lambda x: x.ne(0).sum()).sum(axis=1).reset_index()\n",
    "plt.figure(figsize=figure_size)\n",
    "sns.lineplot(data=qtd_features_zeradas, x='lambda', y=0)\n",
    "plt.xlim(x_lim)\n",
    "plt.ylim([0, 33])\n",
    "plt.ylabel('Coeficientes não zerados')\n",
    "plt.xlabel('Lambda')\n",
    "plt.title('Qual a quantidade de coeficientes zerados com o aumento da regularização Lasso?', size=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por esse gráfico percebemos que algumas variáveis vão a zero muito rapidamente\n",
    "\n",
    "[link](https://afit-r.github.io/regularized_regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature importance com árvores\n",
    "\n",
    "Além de estimadores poderosos, podemos utilizar modelos baseados em árvores para fazer feature selection! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.feature_importances_`\n",
    "\n",
    "Neste caso, o score de importância de cada uma das features é calculado com base na **média e desvio padrão da diminuição de impureza que cada feature proporciona na árvore (ou em cada árvore, no caso de ensembles)**.\n",
    "\n",
    "O método é conhecido como **mean decrease in impurity** (MDI).\n",
    "\n",
    "Este método é rápido, no entanto, o valor é fortemente enviesado para features que têm alta cardinalidade (features numéricas, ou features categóricas com muitos níveis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-11T22:18:51.657887Z",
     "start_time": "2022-03-11T22:18:48.060844Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# instancia e faz o fit do RF\n",
    "rf = RandomForestClassifier(n_estimators=50,\n",
    "                            random_state=42).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-11T22:18:52.475678Z",
     "start_time": "2022-03-11T22:18:52.227797Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# cria series com o nome da feature e o importance\n",
    "feature_importances_rf = pd.Series(rf.feature_importances_, index=rf.feature_names_in_).sort_values(ascending=False)\n",
    "feature_importances_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-11T22:18:53.026387Z",
     "start_time": "2022-03-11T22:18:52.478676Z"
    }
   },
   "outputs": [],
   "source": [
    "# plota o feature importance\n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.title(\"Feature importances using MDI\")\n",
    "plt.barh(feature_importances_rf.index, feature_importances_rf.values)\n",
    "plt.xlabel(\"Mean decrease in impurity\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A partir daqui, o próximo passo seria treinar um modelo normalmente (pipeline, gridsearch, etc.), mas apenas com essas features selecionadas (idealmente, uma combinação dessas heurísticas).\n",
    "\n",
    "Nota: Sempre verifique se você obtém os mesmos resultados com um random_seed diferente antes de interpretar qualquer classificação de importância. Se os resultados mudarem, aumente o número de árvores com `ntree`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Qual método é melhor?\n",
    "Como um bom cientista de dados, você terá que realizar vários experimentos para descobrir qual método é melhor para o seu caso!\n",
    "\n",
    "Mas você pode também usar alguns métodos e selecionar todas as features que foram consideradas importantes por pelo menos 1 ou 2 deles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________________________________________________________________\n",
    "________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliografia e Aprofundamento\n",
    "- [Breve introdução dos métodos supervisionados e não supervisionados de seleção de features](https://www.kdnuggets.com/2021/06/feature-selection-overview.html)\n",
    "- [Lista de estratégias](https://towardsdatascience.com/feature-selection-a-comprehensive-list-of-strategies-3fecdf802b79)\n",
    "- [MUITOS métodos diferentes](https://medium.com/analytics-vidhya/feature-selection-extended-overview-b58f1d524c1c)\n",
    "- [Wrapper methods](https://quantifyinghealth.com/stepwise-selection/)\n",
    "- [Understanding Bias in RF Variable Importance Metrics](https://blog.methodsconsultants.com/posts/be-aware-of-bias-in-rf-variable-importance-metrics/)\n",
    "- [Feature Importance x Permutation Importance](https://scikit-learn.org/stable/auto_examples/inspection/plot_permutation_importance.html#sphx-glr-auto-examples-inspection-plot-permutation-importance-py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________________________________________________________________\n",
    "________________________________________________________________\n",
    "________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Material Complementar: Técnicas Supervisionadas\n",
    "\n",
    "#### Como encontrar o número ideal de features?\n",
    "Método 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load needed libraries\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "from mlxtend.plotting import plot_sequential_feature_selection as plot_sfs\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.datasets import load_boston\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#import and prepare data\n",
    "boston = load_boston()\n",
    "X, y = boston.data, boston.target\n",
    "\n",
    "#Define Sequential Forward Selection (sfs)\n",
    "sfs = SFS(LinearRegression(), \n",
    "          k_features=13, \n",
    "          forward=True, \n",
    "          floating=False, \n",
    "          scoring='neg_mean_squared_error',\n",
    "          cv=5)\n",
    "\n",
    "sfs = sfs.fit(X, y)\n",
    "fig = plot_sfs(sfs.get_metric_dict(), kind='std_err')\n",
    "\n",
    "plt.title('Sequential Forward Selection (w. StdErr)')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "#https://towardsdatascience.com/feature-selection-for-machine-learning-in-python-wrapper-methods-2b5e27d2db31"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Método 2:\n",
    "\n",
    "Adicione uma variável randômica no seu dataset e faça o feature importance. Qualquer feature que tenha uma importância menor que a da feature randômica pode ser desconsiderada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(33)\n",
    "X_train['random_1'] = np.random.randint(-100, 100, size=X_train.shape[0])/100\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=50,\n",
    "                            random_state=42).fit(X_train, y_train)\n",
    "\n",
    "feature_importances_rf = pd.Series(rf.feature_importances_, index=rf.feature_names_in_).sort_values(ascending=False)\n",
    "\n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.title(\"Feature importances using MDI\")\n",
    "plt.barh(feature_importances_rf.index, feature_importances_rf.values)\n",
    "plt.xlabel(\"Mean decrease in impurity\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Material Complementar: Técnicas Não Supervisionadas\n",
    "\n",
    "<img src=\"https://www.kdnuggets.com/wp-content/uploads/Fig4-Butvinik-feature-selection-overview.jpg\" text=\"https://www.kdnuggets.com/2021/06/feature-selection-overview.html\" width=900/>\n",
    "\n",
    "Aqui também podemos separar nossos métodos de acordo com a interação com o modelo:\n",
    "\n",
    "### 1. Filtro\n",
    "\n",
    "Os métodos de filtro podem ser classificados como univariados e multivariados. Da mesma forma que nas técnicas supervisionadas, aqui ordenamos as features de acordo com o método estatístico selecionado. No caso dos univariados, cada feature é analisada separadamente e por isso não é capaz de eliminar features correlacionadas. No multivariado, as features são avaliadas em conjunto em vez de individualmente. \n",
    "\n",
    "### 2. Wrapper\n",
    "\n",
    "O métodos wrapper podem ser divididos em três categorias: sequencial, bio-inspirado e iterativo. \n",
    "\n",
    "Na metodologia sequencial, os recursos são adicionados ou removidos sequencialmente. Na bioinspirada, tenta-se incorporar a aleatoriedade no processo de busca, com o intuito de escapar de ótimos locais. Nos iterativos fazemos uma estimação evitando, assim, uma busca combinatória.\n",
    "\n",
    "Assim como nos métodos wrapper supervisionados, esta abordagem caracteriza-se por encontrar subconjuntos de features que sejam mais relevantes. Porém, ela possui um alto custo computacional.\n",
    "\n",
    "### 3. Metodologia Híbrida\n",
    "\n",
    "Da mesma forma que nos supervisionados, a primeira etapa dessa metodologia consiste em filtrar os recursos mais relevantes para depois aplicar alguma técnica de wrapper.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cb5f626699f206ef97176a4f092b8d9f6e52ae1f84b4bb3163daf9eb25ca3519"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('aula_venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
